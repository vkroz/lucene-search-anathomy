{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Lucene Tutorial Quick look inside Official documentation https://lucene.apache.org/core/7_6_0/ https://lucene.apache.org/core/6_6_5/ Faceted search: http://shaierera.blogspot.com/2012/12/lucene-facets-under-hood.html http://shaierera.blogspot.com/2012/11/lucene-facets-part-1.html http://shaierera.blogspot.com/2012/11/lucene-facets-part-2.html http://lucene.apache.org/core/4_0_0/facet/org/apache/lucene/facet/doc-files/userguide.html#facet_accumulation Lucene NRT segment level replication: http://blog.mikemccandless.com/2017/09/lucenes-near-real-time-segment-index.html Javadocs with class diagrams (doxygen)","title":"Home"},{"location":"#lucene-tutorial","text":"Quick look inside Official documentation https://lucene.apache.org/core/7_6_0/ https://lucene.apache.org/core/6_6_5/ Faceted search: http://shaierera.blogspot.com/2012/12/lucene-facets-under-hood.html http://shaierera.blogspot.com/2012/11/lucene-facets-part-1.html http://shaierera.blogspot.com/2012/11/lucene-facets-part-2.html http://lucene.apache.org/core/4_0_0/facet/org/apache/lucene/facet/doc-files/userguide.html#facet_accumulation Lucene NRT segment level replication: http://blog.mikemccandless.com/2017/09/lucenes-near-real-time-segment-index.html Javadocs with class diagrams (doxygen)","title":"Lucene Tutorial"},{"location":"getting_started/","text":"Getting started Sources https://www.baeldung.com/lucene Lucene Queries","title":"Getting started"},{"location":"getting_started/#lucene-queries","text":"","title":"Lucene Queries"},{"location":"links/","text":"Links Tutorials http://lucene.sourceforge.net/talks/pisa/ https://sease.io/2015/07/exploring-solr-internals-lucene.html http://www.lucenetutorial.com/ Links collection References https://stackoverflow.com/questions/2602253/how-does-lucene-index-documents https://www.quora.com/Could-you-introduce-the-index-file-structure-and-theory-of-Lucene Key implementation classes https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/util/fst/Builder.html https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/codecs/MultiLevelSkipListWriter.html http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html Books collection https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719 https://www.amazon.com/Foundations-Statistical-Natural-Language-Processing/dp/0262133601 https://www.amazon.com/Managing-Gigabytes-Compressing-Multimedia-Information/dp/1558605703 https://www.amazon.com/Information-Retrieval-Implementing-Evaluating-Engines/dp/0262528878","title":"Links"},{"location":"links/#links","text":"","title":"Links"},{"location":"links/#tutorials","text":"http://lucene.sourceforge.net/talks/pisa/ https://sease.io/2015/07/exploring-solr-internals-lucene.html http://www.lucenetutorial.com/","title":"Tutorials"},{"location":"links/#links-collection","text":"","title":"Links collection"},{"location":"links/#references","text":"https://stackoverflow.com/questions/2602253/how-does-lucene-index-documents https://www.quora.com/Could-you-introduce-the-index-file-structure-and-theory-of-Lucene","title":"References"},{"location":"links/#key-implementation-classes","text":"https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/util/fst/Builder.html https://lucene.apache.org/core/6_6_0/core/org/apache/lucene/codecs/MultiLevelSkipListWriter.html http://lucene.apache.org/core/4_0_0/core/org/apache/lucene/codecs/lucene40/Lucene40PostingsFormat.html","title":"Key implementation classes"},{"location":"links/#books-collection","text":"https://www.amazon.com/Introduction-Information-Retrieval-Christopher-Manning/dp/0521865719 https://www.amazon.com/Foundations-Statistical-Natural-Language-Processing/dp/0262133601 https://www.amazon.com/Managing-Gigabytes-Compressing-Multimedia-Information/dp/1558605703 https://www.amazon.com/Information-Retrieval-Implementing-Evaluating-Engines/dp/0262528878","title":"Books collection"},{"location":"look_inside/","text":"Lucene - quick look inside Sources http://lucene.sourceforge.net/talks/pisa/ https://www.youtube.com/watch?v=T5RmMNDR5XI&feature=youtu.be Main points Writing documents Index is represented by Directory object. Could be on file, in memory, database etc IndexWriter writes documents into index. IndexWriterConfig provides writer configuration. Analyzer is a very important part of configuration, both for writing and reading. See http://localhost:63342/code-snippets/lucene-core-7.4.0-javadoc.jar/org/apache/lucene/analysis/Analyzer.html Documents are the unit of indexing and search. A Document is a set of fields . Each field has a name and a textual value. Searching documents IndexReader provides an interface for accessing a point-in-time view of an index. IndexSearcher implements search over a single IndexReader. To search an index: Instantiate QueryParser . This object can be reused multiple times for various queries. When creating QueryParser provide an analyzer similar to how we did for writing an index Create a Query using QueryParser represented by for using with query Submit query instance to indexSearcher.search() and process results. Lucene API org.apache.lucene.document org.apache.lucene.analysis org.apache.lucene.index org.apache.lucene.search Package: org.apache.lucene.document A Document is a sequence of Fields. A Field is a pair. name is the name of the field, e.g., title, body, subject, date, etc. value is text. Field values may be stored, indexed or analyzed (and, now, vectored). Example Document makeDocument ( String title , String body ) { Document doc = new Document (); doc . add ( new TextField ( \"title\" , title , Field . Store . YES )); doc . add ( new TextField ( \"body\" , body , Field . Store . YES )); return doc ; } Package: org.apache.lucene.analysis An Analyzer is a TokenStream factory. A TokenStream is an iterator over Tokens. input is a character iterator (Reader) A Token is tuple text (e.g., \u201cpisa\u201d). type (e.g., \u201cword\u201d, \u201csent\u201d, \u201cpara\u201d). start & length offsets, in characters (e.g, <5,4>) positionIncrement (normally 1) standard TokenStream implementations are Tokenizers , which divide characters into tokens and TokenFilters , e.g., stop lists, stemmers, etc. Example public class ItalianAnalyzer extends Analyzer { private Set stopWords = StopFilter . makeStopSet ( new String [] { \"il\" , \"la\" , \"in\" }; public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new WhitespaceTokenizer ( reader ); result = new LowerCaseFilter ( result ); result = new StopFilter ( result , stopWords ); result = new SnowballFilter ( result , \"Italian\" ); return result ; } } Package: org.apache.lucene.index Term is <fieldName, text> Index maps Term \u2192 <df, <docNum, <position>* >*> e.g. \u201ccontent:pisa\u201d \u2192 <2, <2, <14>>, <4, <2, 9>>> Term vectors Example Example IndexWriter writer = new IndexWriter ( \"index\" , new ItalianAnalyzer ()); File [] files = directory . listFiles (); for ( int i = 0 ; i < files . length ; i ++) { writer . addDocument ( makeDocument ( files [ i ])); } writer . close (); Index API: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/index/package-summary.html Index format and structure is defined by codecs: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/codecs/lucene70/package-summary.html#package.description Some Inverted Index Strategies Strategy Pro Cons Batch-based: use file-sorting algorithms (textbook) - fastest to build - fastest to search slow to update B-tree based: update in place http://lucene.sf.net/papers/sigir90.ps fast to search - update/build does not scale - complex implementation Segment based: lots of small indexes - fast to build - fast to update slower to search Lucene's Index Algorithm two basic algorithms make an index for a single document merge a set of indices incremental algorithm: maintain a stack of segment indices create index for each incoming document push new indexes onto the stack let b=10 be the merge factor; M=\u221e; Example for ( size = 1 ; size < M ; size *= b ) { if ( there are b indexes with size docs on top of the stack ) { pop them off the stack ; merge them into a single index ; push the merged index onto the stack ; } else { break ; } } Todo Need to check/elaborate optimization: single-doc indexes kept in RAM, saves system calls Note average b*logb(N)/2 indexes N=1M, b=2 gives just 20 indexes fast to update and not too slow to search batch indexing w/ M=\u221e, merge all at end equivalent to external merge sort, optimal segment indexing w/ M<\u221e Indexing Diagram b = 3 11 documents indexed stack has four indexes grayed indexes have been deleted 5 merges have occurred Index Compression For keys in Term -> ... map , use technique from Paolo's slides : 4 th solution: front coding Idea: sorted words commonly have long common prefix - stored diffrences only wrt the first term in a block og k For values in Term -> ... map, use technique from Paolo's slides : VInt Encoding Example Value First byte Second byte Third byte 0 00000000 1 00000001 2 00000010 ... 127 01111111 128 10000000 00000001 129 10000001 00000001 130 10000010 00000001 ... 16,383 11111111 01111111 16,384 10000000 10000000 00000001 16,385 10000001 10000000 00000001 ... This provides compression while still being efficient to decode. Package: org.apache.lucene.search primitive queries: TermQuery : match docs containing a Term PhraseQuery : match docs w/ sequence of Terms BooleanQuery : match docs matching other queries. e.g., +path:pisa +content:\u201cDoug Cutting\u201d -path:nutch new: SpansQuery derived queries: PrefixQuery , WildcardQuery , etc. Example Query pisa = new TermQuery ( new Term ( \"content\" , \"pisa\" )); Query babel = new TermQuery ( new Term ( \"content\" , \"babel\" )); PhraseQuery leaningTower = new PhraseQuery (); leaningTower . add ( new Term ( \"content\" , \"leaning\" )); leaningTower . add ( new Term ( \"content\" , \"tower\" )); BooleanQuery query = new BooleanQuery (); query . add ( leaningTower , Occur . MUST ); query . add ( pisa , Occur . SHOULD ); query . add ( babel , Occur . MUST_NOT ); Search algorithms From Paolo's slides : Lucene's Disjunctive Search Algorithm described in http://lucene.sf.net/papers/riao97.ps since all postings must be processed goal is to minimize per-posting computation merges postings through a fixed-size array of accumulator buckets performs boolean logic with bit masks scales well with large queries Todo draw a diagram to illustrate? Lucene's Conjunctive Search Algorithm From Paolo's slides: Algorithm - use linked list of pointers to doc list - initially sorted by doc - loop - if all are at same doc, record hit - skip first to-or-past last and move to end of list Scoring From Paolo's slides: Is very much like Lucene's Similarity Lucene's Phrase Scoring approximate phrase IDF with sum of terms compute actual tf of phrase slop penalizes slight mismatches by edit-distance Physical index formats See: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/codecs/lucene70/package-summary.html","title":"Look inside"},{"location":"look_inside/#lucene-quick-look-inside","text":"Sources http://lucene.sourceforge.net/talks/pisa/ https://www.youtube.com/watch?v=T5RmMNDR5XI&feature=youtu.be","title":"Lucene - quick look inside"},{"location":"look_inside/#main-points","text":"","title":"Main points"},{"location":"look_inside/#writing-documents","text":"Index is represented by Directory object. Could be on file, in memory, database etc IndexWriter writes documents into index. IndexWriterConfig provides writer configuration. Analyzer is a very important part of configuration, both for writing and reading. See http://localhost:63342/code-snippets/lucene-core-7.4.0-javadoc.jar/org/apache/lucene/analysis/Analyzer.html Documents are the unit of indexing and search. A Document is a set of fields . Each field has a name and a textual value.","title":"Writing documents"},{"location":"look_inside/#searching-documents","text":"IndexReader provides an interface for accessing a point-in-time view of an index. IndexSearcher implements search over a single IndexReader. To search an index: Instantiate QueryParser . This object can be reused multiple times for various queries. When creating QueryParser provide an analyzer similar to how we did for writing an index Create a Query using QueryParser represented by for using with query Submit query instance to indexSearcher.search() and process results.","title":"Searching documents"},{"location":"look_inside/#lucene-api","text":"org.apache.lucene.document org.apache.lucene.analysis org.apache.lucene.index org.apache.lucene.search","title":"Lucene API"},{"location":"look_inside/#package-orgapachelucenedocument","text":"A Document is a sequence of Fields. A Field is a pair. name is the name of the field, e.g., title, body, subject, date, etc. value is text. Field values may be stored, indexed or analyzed (and, now, vectored). Example Document makeDocument ( String title , String body ) { Document doc = new Document (); doc . add ( new TextField ( \"title\" , title , Field . Store . YES )); doc . add ( new TextField ( \"body\" , body , Field . Store . YES )); return doc ; }","title":"Package: org.apache.lucene.document"},{"location":"look_inside/#package-orgapacheluceneanalysis","text":"An Analyzer is a TokenStream factory. A TokenStream is an iterator over Tokens. input is a character iterator (Reader) A Token is tuple text (e.g., \u201cpisa\u201d). type (e.g., \u201cword\u201d, \u201csent\u201d, \u201cpara\u201d). start & length offsets, in characters (e.g, <5,4>) positionIncrement (normally 1) standard TokenStream implementations are Tokenizers , which divide characters into tokens and TokenFilters , e.g., stop lists, stemmers, etc. Example public class ItalianAnalyzer extends Analyzer { private Set stopWords = StopFilter . makeStopSet ( new String [] { \"il\" , \"la\" , \"in\" }; public TokenStream tokenStream ( String fieldName , Reader reader ) { TokenStream result = new WhitespaceTokenizer ( reader ); result = new LowerCaseFilter ( result ); result = new StopFilter ( result , stopWords ); result = new SnowballFilter ( result , \"Italian\" ); return result ; } }","title":"Package: org.apache.lucene.analysis"},{"location":"look_inside/#package-orgapacheluceneindex","text":"Term is <fieldName, text> Index maps Term \u2192 <df, <docNum, <position>* >*> e.g. \u201ccontent:pisa\u201d \u2192 <2, <2, <14>>, <4, <2, 9>>> Term vectors Example Example IndexWriter writer = new IndexWriter ( \"index\" , new ItalianAnalyzer ()); File [] files = directory . listFiles (); for ( int i = 0 ; i < files . length ; i ++) { writer . addDocument ( makeDocument ( files [ i ])); } writer . close (); Index API: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/index/package-summary.html Index format and structure is defined by codecs: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/codecs/lucene70/package-summary.html#package.description","title":"Package: org.apache.lucene.index"},{"location":"look_inside/#some-inverted-index-strategies","text":"Strategy Pro Cons Batch-based: use file-sorting algorithms (textbook) - fastest to build - fastest to search slow to update B-tree based: update in place http://lucene.sf.net/papers/sigir90.ps fast to search - update/build does not scale - complex implementation Segment based: lots of small indexes - fast to build - fast to update slower to search","title":"Some Inverted Index Strategies"},{"location":"look_inside/#lucenes-index-algorithm","text":"two basic algorithms make an index for a single document merge a set of indices incremental algorithm: maintain a stack of segment indices create index for each incoming document push new indexes onto the stack let b=10 be the merge factor; M=\u221e; Example for ( size = 1 ; size < M ; size *= b ) { if ( there are b indexes with size docs on top of the stack ) { pop them off the stack ; merge them into a single index ; push the merged index onto the stack ; } else { break ; } } Todo Need to check/elaborate optimization: single-doc indexes kept in RAM, saves system calls Note average b*logb(N)/2 indexes N=1M, b=2 gives just 20 indexes fast to update and not too slow to search batch indexing w/ M=\u221e, merge all at end equivalent to external merge sort, optimal segment indexing w/ M<\u221e","title":"Lucene's Index Algorithm"},{"location":"look_inside/#indexing-diagram","text":"b = 3 11 documents indexed stack has four indexes grayed indexes have been deleted 5 merges have occurred","title":"Indexing Diagram"},{"location":"look_inside/#index-compression","text":"For keys in Term -> ... map , use technique from Paolo's slides : 4 th solution: front coding Idea: sorted words commonly have long common prefix - stored diffrences only wrt the first term in a block og k For values in Term -> ... map, use technique from Paolo's slides : VInt Encoding Example Value First byte Second byte Third byte 0 00000000 1 00000001 2 00000010 ... 127 01111111 128 10000000 00000001 129 10000001 00000001 130 10000010 00000001 ... 16,383 11111111 01111111 16,384 10000000 10000000 00000001 16,385 10000001 10000000 00000001 ... This provides compression while still being efficient to decode.","title":"Index Compression"},{"location":"look_inside/#package-orgapachelucenesearch","text":"primitive queries: TermQuery : match docs containing a Term PhraseQuery : match docs w/ sequence of Terms BooleanQuery : match docs matching other queries. e.g., +path:pisa +content:\u201cDoug Cutting\u201d -path:nutch new: SpansQuery derived queries: PrefixQuery , WildcardQuery , etc. Example Query pisa = new TermQuery ( new Term ( \"content\" , \"pisa\" )); Query babel = new TermQuery ( new Term ( \"content\" , \"babel\" )); PhraseQuery leaningTower = new PhraseQuery (); leaningTower . add ( new Term ( \"content\" , \"leaning\" )); leaningTower . add ( new Term ( \"content\" , \"tower\" )); BooleanQuery query = new BooleanQuery (); query . add ( leaningTower , Occur . MUST ); query . add ( pisa , Occur . SHOULD ); query . add ( babel , Occur . MUST_NOT );","title":"Package: org.apache.lucene.search"},{"location":"look_inside/#search-algorithms","text":"From Paolo's slides :","title":"Search algorithms"},{"location":"look_inside/#lucenes-disjunctive-search-algorithm","text":"described in http://lucene.sf.net/papers/riao97.ps since all postings must be processed goal is to minimize per-posting computation merges postings through a fixed-size array of accumulator buckets performs boolean logic with bit masks scales well with large queries Todo draw a diagram to illustrate?","title":"Lucene's Disjunctive Search Algorithm"},{"location":"look_inside/#lucenes-conjunctive-search-algorithm","text":"From Paolo's slides: Algorithm - use linked list of pointers to doc list - initially sorted by doc - loop - if all are at same doc, record hit - skip first to-or-past last and move to end of list","title":"Lucene's Conjunctive Search Algorithm"},{"location":"look_inside/#scoring","text":"From Paolo's slides: Is very much like Lucene's Similarity","title":"Scoring"},{"location":"look_inside/#lucenes-phrase-scoring","text":"approximate phrase IDF with sum of terms compute actual tf of phrase slop penalizes slight mismatches by edit-distance","title":"Lucene's Phrase Scoring"},{"location":"look_inside/#physical-index-formats","text":"See: https://lucene.apache.org/core/7_6_0/core/org/apache/lucene/codecs/lucene70/package-summary.html","title":"Physical index formats"},{"location":"mkdocs/","text":"Welcome to MkDocs For full documentation visit https://www.mkdocs.org/user-guide/writing-your-docs/ Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start docs server with live-reloading. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy documentation site to git-pages on github mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Formatting elements CodeHilite import tensorflow as tf Shebang Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: 1 2 #!/usr/bin/python import tensorflow as tf Grouping code blocks Note Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); } Admonitions :memo: Title :blue_book: Note: Cool Feature Information here Quote :memo:{.tip} The Tip Title The content Quote Lorem :mega: ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, :blue_book: Note: Cool Feature nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Help Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Faq Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Welcome to MkDocs"},{"location":"mkdocs/#welcome-to-mkdocs","text":"For full documentation visit https://www.mkdocs.org/user-guide/writing-your-docs/","title":"Welcome to MkDocs"},{"location":"mkdocs/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start docs server with live-reloading. mkdocs build - Build the documentation site. mkdocs gh-deploy - Deploy documentation site to git-pages on github mkdocs help - Print this help message. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Commands"},{"location":"mkdocs/#formatting-elements","text":"","title":"Formatting elements"},{"location":"mkdocs/#codehilite","text":"import tensorflow as tf","title":"CodeHilite"},{"location":"mkdocs/#shebang","text":"Alternatively, if the first line of a code block contains a shebang, the language is derived from the path referenced in the shebang. This will only work for code blocks that are indented using four spaces, not for those encapsulated in three backticks. Example: 1 2 #!/usr/bin/python import tensorflow as tf","title":"Shebang"},{"location":"mkdocs/#grouping-code-blocks","text":"Note Bash #!/bin/bash STR = \"Hello World!\" echo $STR C #include int main(void) { printf ( \"hello, world \\n \" ); }","title":"Grouping code blocks"},{"location":"mkdocs/#admonitions","text":":memo: Title :blue_book: Note: Cool Feature Information here Quote :memo:{.tip} The Tip Title The content Quote Lorem :mega: ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, :blue_book: Note: Cool Feature nec semper lorem quam in massa. Note Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Tip Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Help Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Faq Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Admonitions"}]}